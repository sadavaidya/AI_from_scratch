# GPT Decoder from Scratch ğŸ§ âš™ï¸

This implementation demonstrates a simplified GPT decoder model built entirely from scratch using PyTorch. It walks through core transformer building blocks such as self-attention, positional embeddings, and masked attention to generate Shakespeare-like text.

The code is intended for educational purposes and demonstrates how the GPT architecture works under the hood â€” without relying on any high-level libraries.

## ğŸ“ File
- `GPT_from_scratch.ipynb`: Jupyter notebook with fully commented code, training loop, and explanation for each major component.

## ğŸ™ Acknowledgment
This work is inspired by Andrej Karpathy's video on building GPT from scratch, with original restructuring, additional comments, and markdown enhancements added to improve clarity and documentation.
